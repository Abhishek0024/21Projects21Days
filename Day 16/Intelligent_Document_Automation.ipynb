{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Intelligent Document Processing\n",
        "\n",
        "Intelligent Document Processing (IDP) is an advanced automation technology that transforms the way businesses handle documents. By leveraging artificial intelligence (AI), IDP systems can automatically read, understand, and process vast quantities of documents, extracting crucial data and feeding it into various business workflows. This technology is a significant leap from traditional Optical Character Recognition (OCR) as it not only digitizes text but also comprehends the context and structure of the information within documents.\n",
        "\n",
        "At its core, IDP is designed to tackle the challenge of processing unstructured and semi-structured data, which make up a large portion of business documents such as invoices, contracts, emails, and forms. By automating the extraction and interpretation of this data, IDP significantly reduces the need for manual data entry, leading to increased efficiency, higher accuracy, and substantial cost savings."
      ],
      "metadata": {
        "id": "_-Vt-KZu0SZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install -y poppler-utils\n",
        "\n",
        "! pip install opencv-python matplotlib numpy pdf2image\n",
        "! pip install poppler-utils\n",
        "! pip install pytesseract pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwILzUUy0Noj",
        "outputId": "cda6cb62-0b48-49fc-fc05-7ddfce7eb9e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 38 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 697 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.11 [186 kB]\n",
            "Fetched 186 kB in 1s (151 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 126718 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.11_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.11) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.11) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pdf2image\n",
            "Successfully installed pdf2image-1.17.0\n",
            "Collecting poppler-utils\n",
            "  Downloading poppler_utils-0.1.0-py3-none-any.whl.metadata (883 bytes)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.12/dist-packages (from poppler-utils) (8.3.0)\n",
            "Downloading poppler_utils-0.1.0-py3-none-any.whl (9.2 kB)\n",
            "Installing collected packages: poppler-utils\n",
            "Successfully installed poppler-utils-0.1.0\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from pytesseract) (25.0)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ObrOkJd30T8K"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_image(image, title=\"Image\"):\n",
        "    plt.figure(figsize=(7, 7))\n",
        "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Kd_QarTR0Zl-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grayscale Conversion"
      ],
      "metadata": {
        "id": "U_82iifx4m3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the image to grayscale\n",
        "def convert_to_grayscale(image):\n",
        "  return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "def reduce_noise(gray_image):\n",
        "  return cv2.GaussianBlur(gray_image, (5, 5), 0)"
      ],
      "metadata": {
        "id": "lkUqFzLm0elj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Binarization (Thresholding)"
      ],
      "metadata": {
        "id": "aFMDJOw14cxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def binarize_image(blur_reduced_image):\n",
        "  return cv2.adaptiveThreshold(\n",
        "    blur_reduced_image,\n",
        "    255,\n",
        "    cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "    cv2.THRESH_BINARY_INV, # Invert the colors (text becomes white)\n",
        "    11, # Block size\n",
        "    4  # Constant C\n",
        "  )"
      ],
      "metadata": {
        "id": "yTtF7uGQ0jWO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Skew Correction"
      ],
      "metadata": {
        "id": "efWdG1tY4WUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def deskew_image(image):\n",
        "    \"\"\"\n",
        "    Corrects the skew of an image by finding the minimum area rectangle\n",
        "    of the text block and rotating accordingly.\n",
        "    \"\"\"\n",
        "    # Find all non-zero (white) pixels\n",
        "    coords = cv2.findNonZero(image)\n",
        "\n",
        "    # Get the minimum area bounding rectangle\n",
        "    # It returns (center(x,y), (width, height), angle of rotation)\n",
        "    rect = cv2.minAreaRect(coords)\n",
        "    angle = rect[-1] - 90\n",
        "\n",
        "    # The `cv2.minAreaRect` angle has a specific range.\n",
        "    # We need to adjust it for our rotation.\n",
        "    if angle < -45:\n",
        "        angle = -(90 + angle)\n",
        "    else:\n",
        "        angle = angle\n",
        "\n",
        "    # Get the rotation matrix and rotate the image\n",
        "    (h, w) = image.shape[:2]\n",
        "    center = (w // 2, h // 2)\n",
        "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
        "    rotated = cv2.warpAffine(image, M, (w, h),\n",
        "                             flags=cv2.INTER_CUBIC,\n",
        "                             borderMode=cv2.BORDER_REPLICATE)\n",
        "    print(f\"Detected skew angle: {angle:.2f} degrees\")\n",
        "\n",
        "    # Now, rotate the original grayscale image by the same angle\n",
        "    (h, w) = rotated.shape\n",
        "    center = (w // 2, h // 2)\n",
        "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
        "    deskewed_gray = cv2.warpAffine(rotated, M, (w, h),\n",
        "                                  flags=cv2.INTER_CUBIC,\n",
        "                                  borderMode=cv2.BORDER_REPLICATE)\n",
        "\n",
        "    return deskewed_gray"
      ],
      "metadata": {
        "id": "pC6wgvwk0oaq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_one_image(image):\n",
        "  image = convert_to_grayscale(image)\n",
        "  print(\"Converted image to grayscale..\")\n",
        "  image = reduce_noise(image)\n",
        "  print(\"Reduced noise in the image..\")\n",
        "  image = binarize_image(image)\n",
        "  print(\"Binarized the image..\")\n",
        "  image = deskew_image(image)\n",
        "  print(\"Corrected image orientation..\")\n",
        "  return image"
      ],
      "metadata": {
        "id": "ejrgRQHD0q5A"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting the data"
      ],
      "metadata": {
        "id": "OYU9hFJWEn1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Step 1: Download the dataset\n",
        "path = kagglehub.dataset_download(\"snehaanbhawal/resume-dataset\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "# Step 2: Create an output directory for the 20 sampled resumes\n",
        "output_dir = '/content/Resumes'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Step 3: Find all PDF files in the dataset\n",
        "pdf_files = []\n",
        "for root, dirs, files in os.walk(path):\n",
        "    for file in files:\n",
        "        if file.lower().endswith(\".pdf\"):\n",
        "            pdf_files.append(os.path.join(root, file))\n",
        "\n",
        "# Step 4: Select only the first 20 PDF files\n",
        "sample_pdfs = pdf_files[:20]\n",
        "\n",
        "# Step 5: Copy the selected files into the output directory\n",
        "for pdf in sample_pdfs:\n",
        "    shutil.copy(pdf, output_dir)\n",
        "\n",
        "print(f\"✅ Extracted {len(sample_pdfs)} PDF resumes to: {output_dir}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-Y9vRv72fz8",
        "outputId": "d51cf89f-f932-4da8-bbab-2bb0c2c2de29"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'resume-dataset' dataset.\n",
            "Path to dataset files: /kaggle/input/resume-dataset\n",
            "✅ Extracted 20 PDF resumes to: /content/Resumes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepping the resumes for extraction:"
      ],
      "metadata": {
        "id": "Py2MQFlA0xSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import os\n",
        "import zipfile\n",
        "from pdf2image import convert_from_path\n",
        "\n",
        "output_folder_path = \"/content/processed_images\"\n",
        "\n",
        "if os.makedirs(output_folder_path, exist_ok=True):\n",
        "  print(f\"Created folder: {output_folder_path}\")\n",
        "\n",
        "resumes_folder = '/content/Resumes'\n",
        "\n",
        "for resume_name in os.listdir(resumes_folder):\n",
        "  if resume_name.endswith('.pdf'):\n",
        "    print(f\"Processing resume: {resume_name}\")\n",
        "    resume_path = os.path.join(resumes_folder, resume_name)\n",
        "\n",
        "    # Convert the first page of the PDF to an image\n",
        "    try:\n",
        "      pages = convert_from_path(resume_path, first_page=1, last_page=1)\n",
        "      if pages:\n",
        "        image = cv2.cvtColor(np.array(pages[0]), cv2.COLOR_RGB2BGR)\n",
        "        processed_image = process_one_image(image)\n",
        "        output_path = os.path.join(output_folder_path, resume_name.replace('.pdf', '.png'))\n",
        "        cv2.imwrite(output_path, processed_image)\n",
        "        print(f\"Saved processed image to: {output_path}\")\n",
        "        print(\"-\"*50)\n",
        "      else:\n",
        "        print(f\"Could not convert the first page of {resume_name} to an image.\")\n",
        "        print(\"-\"*50)\n",
        "    except Exception as e:\n",
        "      print(f\"Error processing {resume_name}: {e}\")\n",
        "      print(\"-\"*50)\n",
        "\n",
        "\n",
        "print(\"Processing images is completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIn3iEj_0t9n",
        "outputId": "a14d6575-b088-48b6-aedf-84f120b6ab0b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing resume: 34657584.pdf\n",
            "Converted image to grayscale..\n",
            "Reduced noise in the image..\n",
            "Binarized the image..\n",
            "Detected skew angle: -0.03 degrees\n",
            "Corrected image orientation..\n",
            "Saved processed image to: /content/processed_images/34657584.png\n",
            "--------------------------------------------------\n",
            "Processing resume: 41506705.pdf\n",
            "Converted image to grayscale..\n",
            "Reduced noise in the image..\n",
            "Binarized the image..\n",
            "Detected skew angle: -0.21 degrees\n",
            "Corrected image orientation..\n",
            "Saved processed image to: /content/processed_images/41506705.png\n",
            "--------------------------------------------------\n",
            "Processing resume: 22506245.pdf\n",
            "Converted image to grayscale..\n",
            "Reduced noise in the image..\n",
            "Binarized the image..\n",
            "Detected skew angle: -0.00 degrees\n",
            "Corrected image orientation..\n",
            "Saved processed image to: /content/processed_images/22506245.png\n",
            "--------------------------------------------------\n",
            "Processing resume: 11807040.pdf\n",
            "Converted image to grayscale..\n",
            "Reduced noise in the image..\n",
            "Binarized the image..\n",
            "Detected skew angle: -0.00 degrees\n",
            "Corrected image orientation..\n",
            "Saved processed image to: /content/processed_images/11807040.png\n",
            "--------------------------------------------------\n",
            "Processing resume: 85101052.pdf\n",
            "Converted image to grayscale..\n",
            "Reduced noise in the image..\n",
            "Binarized the image..\n",
            "Detected skew angle: 0.00 degrees\n",
            "Corrected image orientation..\n",
            "Saved processed image to: /content/processed_images/85101052.png\n",
            "--------------------------------------------------\n",
            "Processing resume: 12674307.pdf\n",
            "Converted image to grayscale..\n",
            "Reduced noise in the image..\n",
            "Binarized the image..\n",
            "Detected skew angle: -0.00 degrees\n",
            "Corrected image orientation..\n",
            "Saved processed image to: /content/processed_images/12674307.png\n",
            "--------------------------------------------------\n",
            "Processing resume: 27497542.pdf\n",
            "Converted image to grayscale..\n",
            "Reduced noise in the image..\n",
            "Binarized the image..\n",
            "Detected skew angle: -0.07 degrees\n",
            "Corrected image orientation..\n",
            "Saved processed image to: /content/processed_images/27497542.png\n",
            "--------------------------------------------------\n",
            "Processing resume: 34349255.pdf\n",
            "Converted image to grayscale..\n",
            "Reduced noise in the image..\n",
            "Binarized the image..\n",
            "Detected skew angle: -0.00 degrees\n",
            "Corrected image orientation..\n",
            "Saved processed image to: /content/processed_images/34349255.png\n",
            "--------------------------------------------------\n",
            "Processing resume: 13998435.pdf\n",
            "Converted image to grayscale..\n",
            "Reduced noise in the image..\n",
            "Binarized the image..\n",
            "Detected skew angle: -0.00 degrees\n",
            "Corrected image orientation..\n",
            "Saved processed image to: /content/processed_images/13998435.png\n",
            "--------------------------------------------------\n",
            "Processing resume: 38744475.pdf\n",
            "Converted image to grayscale..\n",
            "Reduced noise in the image..\n",
            "Binarized the image..\n",
            "Detected skew angle: -0.00 degrees\n",
            "Corrected image orientation..\n",
            "Saved processed image to: /content/processed_images/38744475.png\n",
            "--------------------------------------------------\n",
            "Processing resume: 51681660.pdf\n",
            "Converted image to grayscale..\n",
            "Reduced noise in the image..\n",
            "Binarized the image..\n",
            "Detected skew angle: 0.00 degrees\n",
            "Corrected image orientation..\n",
            "Saved processed image to: /content/processed_images/51681660.png\n",
            "--------------------------------------------------\n",
            "Processing resume: 38565119.pdf\n",
            "Converted image to grayscale..\n",
            "Reduced noise in the image..\n",
            "Binarized the image..\n",
            "Detected skew angle: 0.00 degrees\n",
            "Corrected image orientation..\n",
            "Saved processed image to: /content/processed_images/38565119.png\n",
            "--------------------------------------------------\n",
            "Processing resume: 17555081.pdf\n",
            "Converted image to grayscale..\n",
            "Reduced noise in the image..\n",
            "Binarized the image..\n",
            "Detected skew angle: -0.00 degrees\n",
            "Corrected image orientation..\n",
            "Saved processed image to: /content/processed_images/17555081.png\n",
            "--------------------------------------------------\n",
            "Processing resume: 26790545.pdf\n",
            "Converted image to grayscale..\n",
            "Reduced noise in the image..\n",
            "Binarized the image..\n",
            "Detected skew angle: -0.05 degrees\n",
            "Corrected image orientation..\n",
            "Saved processed image to: /content/processed_images/26790545.png\n",
            "--------------------------------------------------\n",
            "Processing resume: 11155153.pdf\n",
            "Converted image to grayscale..\n",
            "Reduced noise in the image..\n",
            "Binarized the image..\n",
            "Detected skew angle: 0.00 degrees\n",
            "Corrected image orientation..\n",
            "Saved processed image to: /content/processed_images/11155153.png\n",
            "--------------------------------------------------\n",
            "Processing resume: 44145704.pdf\n",
            "Converted image to grayscale..\n",
            "Reduced noise in the image..\n",
            "Binarized the image..\n",
            "Detected skew angle: -0.00 degrees\n",
            "Corrected image orientation..\n",
            "Saved processed image to: /content/processed_images/44145704.png\n",
            "--------------------------------------------------\n",
            "Processing resume: 24583187.pdf\n",
            "Converted image to grayscale..\n",
            "Reduced noise in the image..\n",
            "Binarized the image..\n",
            "Detected skew angle: -0.03 degrees\n",
            "Corrected image orientation..\n",
            "Saved processed image to: /content/processed_images/24583187.png\n",
            "--------------------------------------------------\n",
            "Processing resume: 67582956.pdf\n",
            "Converted image to grayscale..\n",
            "Reduced noise in the image..\n",
            "Binarized the image..\n",
            "Detected skew angle: -0.00 degrees\n",
            "Corrected image orientation..\n",
            "Saved processed image to: /content/processed_images/67582956.png\n",
            "--------------------------------------------------\n",
            "Processing resume: 90066849.pdf\n",
            "Converted image to grayscale..\n",
            "Reduced noise in the image..\n",
            "Binarized the image..\n",
            "Detected skew angle: -0.00 degrees\n",
            "Corrected image orientation..\n",
            "Saved processed image to: /content/processed_images/90066849.png\n",
            "--------------------------------------------------\n",
            "Processing resume: 54201930.pdf\n",
            "Converted image to grayscale..\n",
            "Reduced noise in the image..\n",
            "Binarized the image..\n",
            "Detected skew angle: 0.00 degrees\n",
            "Corrected image orientation..\n",
            "Saved processed image to: /content/processed_images/54201930.png\n",
            "--------------------------------------------------\n",
            "Processing images is completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Extraction: Tesseract"
      ],
      "metadata": {
        "id": "hYGIcW-e3Upe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import pytesseract\n",
        "import time\n",
        "\n",
        "input_folder_path = \"/content/processed_images\"\n",
        "output_folder_path = \"/content/tesseract_output\"\n",
        "start_time = time.time()\n",
        "\n",
        "if os.makedirs(output_folder_path, exist_ok=True):\n",
        "  print(f\"Created folder: {output_folder_path}\")\n",
        "\n",
        "total_images = sum(1 for entry in os.scandir(input_folder_path))\n",
        "print(f\"Total images in folder: {total_images}\")\n",
        "\n",
        "for i, image_name in enumerate(os.listdir(input_folder_path)[:20], 1):\n",
        "  print(f\"Processing image {i}/{total_images}: {image_name}\")\n",
        "  image_path = os.path.join(input_folder_path, image_name)\n",
        "  print(\"Extracting text from image..\")\n",
        "  text = pytesseract.image_to_string(Image.open(image_path))\n",
        "  output_path = os.path.join(output_folder_path, image_name.replace(\".png\", \".txt\"))\n",
        "  with open(output_path, \"w\") as f:\n",
        "    f.write(text)\n",
        "\n",
        "  print(f\"Saved extracted text to {output_path}\")\n",
        "  print(\"-\"*50)\n",
        "\n",
        "print(\"Text Extraction Completed.\")\n",
        "print(f\"Total time taken: {time.time() - start_time} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YHtMvmQ3NKP",
        "outputId": "e58bf1bf-20bc-4edf-b323-5a3d57a8721e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images in folder: 20\n",
            "Processing image 1/20: 26790545.png\n",
            "Extracting text from image..\n",
            "Saved extracted text to /content/tesseract_output/26790545.txt\n",
            "--------------------------------------------------\n",
            "Processing image 2/20: 27497542.png\n",
            "Extracting text from image..\n",
            "Saved extracted text to /content/tesseract_output/27497542.txt\n",
            "--------------------------------------------------\n",
            "Processing image 3/20: 12674307.png\n",
            "Extracting text from image..\n",
            "Saved extracted text to /content/tesseract_output/12674307.txt\n",
            "--------------------------------------------------\n",
            "Processing image 4/20: 38565119.png\n",
            "Extracting text from image..\n",
            "Saved extracted text to /content/tesseract_output/38565119.txt\n",
            "--------------------------------------------------\n",
            "Processing image 5/20: 24583187.png\n",
            "Extracting text from image..\n",
            "Saved extracted text to /content/tesseract_output/24583187.txt\n",
            "--------------------------------------------------\n",
            "Processing image 6/20: 38744475.png\n",
            "Extracting text from image..\n",
            "Saved extracted text to /content/tesseract_output/38744475.txt\n",
            "--------------------------------------------------\n",
            "Processing image 7/20: 54201930.png\n",
            "Extracting text from image..\n",
            "Saved extracted text to /content/tesseract_output/54201930.txt\n",
            "--------------------------------------------------\n",
            "Processing image 8/20: 11155153.png\n",
            "Extracting text from image..\n",
            "Saved extracted text to /content/tesseract_output/11155153.txt\n",
            "--------------------------------------------------\n",
            "Processing image 9/20: 22506245.png\n",
            "Extracting text from image..\n",
            "Saved extracted text to /content/tesseract_output/22506245.txt\n",
            "--------------------------------------------------\n",
            "Processing image 10/20: 85101052.png\n",
            "Extracting text from image..\n",
            "Saved extracted text to /content/tesseract_output/85101052.txt\n",
            "--------------------------------------------------\n",
            "Processing image 11/20: 67582956.png\n",
            "Extracting text from image..\n",
            "Saved extracted text to /content/tesseract_output/67582956.txt\n",
            "--------------------------------------------------\n",
            "Processing image 12/20: 41506705.png\n",
            "Extracting text from image..\n",
            "Saved extracted text to /content/tesseract_output/41506705.txt\n",
            "--------------------------------------------------\n",
            "Processing image 13/20: 44145704.png\n",
            "Extracting text from image..\n",
            "Saved extracted text to /content/tesseract_output/44145704.txt\n",
            "--------------------------------------------------\n",
            "Processing image 14/20: 51681660.png\n",
            "Extracting text from image..\n",
            "Saved extracted text to /content/tesseract_output/51681660.txt\n",
            "--------------------------------------------------\n",
            "Processing image 15/20: 13998435.png\n",
            "Extracting text from image..\n",
            "Saved extracted text to /content/tesseract_output/13998435.txt\n",
            "--------------------------------------------------\n",
            "Processing image 16/20: 11807040.png\n",
            "Extracting text from image..\n",
            "Saved extracted text to /content/tesseract_output/11807040.txt\n",
            "--------------------------------------------------\n",
            "Processing image 17/20: 17555081.png\n",
            "Extracting text from image..\n",
            "Saved extracted text to /content/tesseract_output/17555081.txt\n",
            "--------------------------------------------------\n",
            "Processing image 18/20: 90066849.png\n",
            "Extracting text from image..\n",
            "Saved extracted text to /content/tesseract_output/90066849.txt\n",
            "--------------------------------------------------\n",
            "Processing image 19/20: 34349255.png\n",
            "Extracting text from image..\n",
            "Saved extracted text to /content/tesseract_output/34349255.txt\n",
            "--------------------------------------------------\n",
            "Processing image 20/20: 34657584.png\n",
            "Extracting text from image..\n",
            "Saved extracted text to /content/tesseract_output/34657584.txt\n",
            "--------------------------------------------------\n",
            "Text Extraction Completed.\n",
            "Total time taken: 340.9297592639923 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Information Extraction"
      ],
      "metadata": {
        "id": "LVCDKIct4_hs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Extract key information from the given resume text.\n",
        "Information to be extracted: Position, skills, summary, work_experience.\n",
        "\n",
        "The text has been extracted from a resume using Tesseract OCR. Use only this text to extract information.\n",
        "Do NOT make up or generate any data. If a field is not present in the text, leave it as a blank string (\"\").\n",
        "\n",
        "For the \"work_experience\" field, summarize the person's experience into a short paragraph highlighting their key roles, achievements, and duration, based only on the extracted text.\n",
        "\n",
        "Always give your response in the following JSON format:\n",
        "\n",
        "{\n",
        "    \"Position\": \"\",\n",
        "    \"skills\": \"\",\n",
        "    \"summary\": \"\",\n",
        "    \"work_experience\": \"\"\n",
        "}\n",
        "\n",
        "Respond strictly in the specified JSON format without adding any extra commentary or explanation.\n",
        "\n",
        "Here is the extracted text:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "_3oDwecN4GpJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.colab import userdata\n",
        "from PIL import Image\n",
        "import json\n",
        "import time\n",
        "\n",
        "genai_client = genai.Client(api_key=userdata.get('GOOGLE_API_KEY'))"
      ],
      "metadata": {
        "id": "Tlza_jz94KX3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "from PIL import Image\n",
        "\n",
        "image_folder_path = \"/content/processed_images\"\n",
        "text_folder_path = \"/content/tesseract_output\"\n",
        "output_folder_path = \"/content/json_output\"\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "os.makedirs(output_folder_path, exist_ok=True)\n",
        "print(f\"Ensured folder exists: {output_folder_path}\")\n",
        "\n",
        "total_images = sum(1 for entry in os.scandir(image_folder_path))\n",
        "print(f\"Total images in folder: {total_images}\")\n",
        "\n",
        "for i, image_name in enumerate(os.listdir(image_folder_path)[:20], 1):\n",
        "    print(f\"Processing image {i}/{total_images}: {image_name}\")\n",
        "    image_path = os.path.join(image_folder_path, image_name)\n",
        "    print(f\"Loading image: {image_path}\")\n",
        "\n",
        "    with open(image_path, \"rb\") as f:\n",
        "        image = Image.open(image_path)\n",
        "\n",
        "    # Handle both .png and .jpg\n",
        "    base_name, _ = os.path.splitext(image_name)\n",
        "    text_path = os.path.join(text_folder_path, base_name + \".txt\")\n",
        "\n",
        "    print(f\"Loading extracted text: {text_path}\")\n",
        "    with open(text_path, \"r\") as f:\n",
        "        text = f.read()\n",
        "\n",
        "    print(\"Extracting information from image and text..\")\n",
        "\n",
        "    prompt_with_text = prompt + text\n",
        "\n",
        "    contents = [\n",
        "        image,\n",
        "        {\"text\": prompt_with_text}\n",
        "    ]\n",
        "    response = genai_client.models.generate_content(\n",
        "        model='gemini-2.5-flash',\n",
        "        contents=contents\n",
        "    )\n",
        "\n",
        "    # Access the usage_metadata attribute\n",
        "    usage_metadata = response.usage_metadata\n",
        "    print(f\"Input Token Count: {usage_metadata.prompt_token_count}\")\n",
        "    print(f\"Thoughts Token Count: {response.usage_metadata.thoughts_token_count}\")\n",
        "    print(f\"Output Token Count: {usage_metadata.candidates_token_count}\")\n",
        "    print(f\"Total Token Count: {usage_metadata.total_token_count}\")\n",
        "\n",
        "    # ---- Safe response parsing ----\n",
        "    response_text = None\n",
        "    if hasattr(response, \"text\") and response.text:\n",
        "        response_text = response.text\n",
        "    elif hasattr(response, \"candidates\") and response.candidates:\n",
        "        parts = response.candidates[0].content.parts\n",
        "        if parts and hasattr(parts[0], \"text\"):\n",
        "            response_text = parts[0].text\n",
        "\n",
        "    if response_text is None:\n",
        "        print(\"⚠️ No text returned from model. Skipping this file.\")\n",
        "        continue\n",
        "\n",
        "    # Clean and parse JSON safely\n",
        "    response_text = response_text.replace('```json', '').replace('```', '')\n",
        "\n",
        "    try:\n",
        "        extracted_information = json.loads(response_text)\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"⚠️ Failed to decode JSON for {image_name}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # Save JSON with correct name\n",
        "    output_path = os.path.join(output_folder_path, base_name + \".json\")\n",
        "    with open(output_path, \"w\") as f:\n",
        "        json.dump(extracted_information, f, indent=4)\n",
        "\n",
        "    print(f\"Saved extracted information to {output_path}\")\n",
        "    print(\"-\" * 50)\n",
        "    time.sleep(60)\n",
        "\n",
        "print(\"Information Extraction Completed.\")\n",
        "print(f\"Total time taken: {time.time() - start_time} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hxq6RU1X4MOG",
        "outputId": "6f81ecb3-4ef7-4219-aa37-84027e376fc5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensured folder exists: /content/json_output\n",
            "Total images in folder: 20\n",
            "Processing image 1/20: 26790545.png\n",
            "Loading image: /content/processed_images/26790545.png\n",
            "Loading extracted text: /content/tesseract_output/26790545.txt\n",
            "Extracting information from image and text..\n",
            "Input Token Count: 1295\n",
            "Thoughts Token Count: 2120\n",
            "Output Token Count: 404\n",
            "Total Token Count: 3819\n",
            "Saved extracted information to /content/json_output/26790545.json\n",
            "--------------------------------------------------\n",
            "Processing image 2/20: 27497542.png\n",
            "Loading image: /content/processed_images/27497542.png\n",
            "Loading extracted text: /content/tesseract_output/27497542.txt\n",
            "Extracting information from image and text..\n",
            "Input Token Count: 1106\n",
            "Thoughts Token Count: 1485\n",
            "Output Token Count: 309\n",
            "Total Token Count: 2900\n",
            "Saved extracted information to /content/json_output/27497542.json\n",
            "--------------------------------------------------\n",
            "Processing image 3/20: 12674307.png\n",
            "Loading image: /content/processed_images/12674307.png\n",
            "Loading extracted text: /content/tesseract_output/12674307.txt\n",
            "Extracting information from image and text..\n",
            "Input Token Count: 986\n",
            "Thoughts Token Count: 836\n",
            "Output Token Count: 322\n",
            "Total Token Count: 2144\n",
            "Saved extracted information to /content/json_output/12674307.json\n",
            "--------------------------------------------------\n",
            "Processing image 4/20: 38565119.png\n",
            "Loading image: /content/processed_images/38565119.png\n",
            "Loading extracted text: /content/tesseract_output/38565119.txt\n",
            "Extracting information from image and text..\n",
            "Input Token Count: 1190\n",
            "Thoughts Token Count: 1739\n",
            "Output Token Count: 388\n",
            "Total Token Count: 3317\n",
            "Saved extracted information to /content/json_output/38565119.json\n",
            "--------------------------------------------------\n",
            "Processing image 5/20: 24583187.png\n",
            "Loading image: /content/processed_images/24583187.png\n",
            "Loading extracted text: /content/tesseract_output/24583187.txt\n",
            "Extracting information from image and text..\n",
            "Input Token Count: 1489\n",
            "Thoughts Token Count: 2723\n",
            "Output Token Count: 469\n",
            "Total Token Count: 4681\n",
            "Saved extracted information to /content/json_output/24583187.json\n",
            "--------------------------------------------------\n",
            "Processing image 6/20: 38744475.png\n",
            "Loading image: /content/processed_images/38744475.png\n",
            "Loading extracted text: /content/tesseract_output/38744475.txt\n",
            "Extracting information from image and text..\n",
            "Input Token Count: 1115\n",
            "Thoughts Token Count: 1707\n",
            "Output Token Count: 464\n",
            "Total Token Count: 3286\n",
            "Saved extracted information to /content/json_output/38744475.json\n",
            "--------------------------------------------------\n",
            "Processing image 7/20: 54201930.png\n",
            "Loading image: /content/processed_images/54201930.png\n",
            "Loading extracted text: /content/tesseract_output/54201930.txt\n",
            "Extracting information from image and text..\n",
            "Input Token Count: 1483\n",
            "Thoughts Token Count: 1375\n",
            "Output Token Count: 429\n",
            "Total Token Count: 3287\n",
            "Saved extracted information to /content/json_output/54201930.json\n",
            "--------------------------------------------------\n",
            "Processing image 8/20: 11155153.png\n",
            "Loading image: /content/processed_images/11155153.png\n",
            "Loading extracted text: /content/tesseract_output/11155153.txt\n",
            "Extracting information from image and text..\n",
            "Input Token Count: 1494\n",
            "Thoughts Token Count: 1952\n",
            "Output Token Count: 594\n",
            "Total Token Count: 4040\n",
            "Saved extracted information to /content/json_output/11155153.json\n",
            "--------------------------------------------------\n",
            "Processing image 9/20: 22506245.png\n",
            "Loading image: /content/processed_images/22506245.png\n",
            "Loading extracted text: /content/tesseract_output/22506245.txt\n",
            "Extracting information from image and text..\n",
            "Input Token Count: 1276\n",
            "Thoughts Token Count: 1018\n",
            "Output Token Count: 420\n",
            "Total Token Count: 2714\n",
            "Saved extracted information to /content/json_output/22506245.json\n",
            "--------------------------------------------------\n",
            "Processing image 10/20: 85101052.png\n",
            "Loading image: /content/processed_images/85101052.png\n",
            "Loading extracted text: /content/tesseract_output/85101052.txt\n",
            "Extracting information from image and text..\n",
            "Input Token Count: 1516\n",
            "Thoughts Token Count: 2367\n",
            "Output Token Count: 430\n",
            "Total Token Count: 4313\n",
            "Saved extracted information to /content/json_output/85101052.json\n",
            "--------------------------------------------------\n",
            "Processing image 11/20: 67582956.png\n",
            "Loading image: /content/processed_images/67582956.png\n",
            "Loading extracted text: /content/tesseract_output/67582956.txt\n",
            "Extracting information from image and text..\n",
            "Input Token Count: 1227\n",
            "Thoughts Token Count: 347\n",
            "Output Token Count: 351\n",
            "Total Token Count: 1925\n",
            "Saved extracted information to /content/json_output/67582956.json\n",
            "--------------------------------------------------\n",
            "Processing image 12/20: 41506705.png\n",
            "Loading image: /content/processed_images/41506705.png\n",
            "Loading extracted text: /content/tesseract_output/41506705.txt\n",
            "Extracting information from image and text..\n",
            "Input Token Count: 1310\n",
            "Thoughts Token Count: 2806\n",
            "Output Token Count: 512\n",
            "Total Token Count: 4628\n",
            "Saved extracted information to /content/json_output/41506705.json\n",
            "--------------------------------------------------\n",
            "Processing image 13/20: 44145704.png\n",
            "Loading image: /content/processed_images/44145704.png\n",
            "Loading extracted text: /content/tesseract_output/44145704.txt\n",
            "Extracting information from image and text..\n",
            "Input Token Count: 1080\n",
            "Thoughts Token Count: 2639\n",
            "Output Token Count: 451\n",
            "Total Token Count: 4170\n",
            "Saved extracted information to /content/json_output/44145704.json\n",
            "--------------------------------------------------\n",
            "Processing image 14/20: 51681660.png\n",
            "Loading image: /content/processed_images/51681660.png\n",
            "Loading extracted text: /content/tesseract_output/51681660.txt\n",
            "Extracting information from image and text..\n",
            "Input Token Count: 1268\n",
            "Thoughts Token Count: 1156\n",
            "Output Token Count: 381\n",
            "Total Token Count: 2805\n",
            "Saved extracted information to /content/json_output/51681660.json\n",
            "--------------------------------------------------\n",
            "Processing image 15/20: 13998435.png\n",
            "Loading image: /content/processed_images/13998435.png\n",
            "Loading extracted text: /content/tesseract_output/13998435.txt\n",
            "Extracting information from image and text..\n",
            "Input Token Count: 1247\n",
            "Thoughts Token Count: 1432\n",
            "Output Token Count: 422\n",
            "Total Token Count: 3101\n",
            "Saved extracted information to /content/json_output/13998435.json\n",
            "--------------------------------------------------\n",
            "Processing image 16/20: 11807040.png\n",
            "Loading image: /content/processed_images/11807040.png\n",
            "Loading extracted text: /content/tesseract_output/11807040.txt\n",
            "Extracting information from image and text..\n",
            "Input Token Count: 1377\n",
            "Thoughts Token Count: 758\n",
            "Output Token Count: 300\n",
            "Total Token Count: 2435\n",
            "Saved extracted information to /content/json_output/11807040.json\n",
            "--------------------------------------------------\n",
            "Processing image 17/20: 17555081.png\n",
            "Loading image: /content/processed_images/17555081.png\n",
            "Loading extracted text: /content/tesseract_output/17555081.txt\n",
            "Extracting information from image and text..\n",
            "Input Token Count: 1274\n",
            "Thoughts Token Count: 1123\n",
            "Output Token Count: 373\n",
            "Total Token Count: 2770\n",
            "Saved extracted information to /content/json_output/17555081.json\n",
            "--------------------------------------------------\n",
            "Processing image 18/20: 90066849.png\n",
            "Loading image: /content/processed_images/90066849.png\n",
            "Loading extracted text: /content/tesseract_output/90066849.txt\n",
            "Extracting information from image and text..\n",
            "Input Token Count: 1409\n",
            "Thoughts Token Count: 2764\n",
            "Output Token Count: 724\n",
            "Total Token Count: 4897\n",
            "Saved extracted information to /content/json_output/90066849.json\n",
            "--------------------------------------------------\n",
            "Processing image 19/20: 34349255.png\n",
            "Loading image: /content/processed_images/34349255.png\n",
            "Loading extracted text: /content/tesseract_output/34349255.txt\n",
            "Extracting information from image and text..\n",
            "Input Token Count: 937\n",
            "Thoughts Token Count: 2814\n",
            "Output Token Count: 250\n",
            "Total Token Count: 4001\n",
            "Saved extracted information to /content/json_output/34349255.json\n",
            "--------------------------------------------------\n",
            "Processing image 20/20: 34657584.png\n",
            "Loading image: /content/processed_images/34657584.png\n",
            "Loading extracted text: /content/tesseract_output/34657584.txt\n",
            "Extracting information from image and text..\n",
            "Input Token Count: 1378\n",
            "Thoughts Token Count: 2882\n",
            "Output Token Count: 380\n",
            "Total Token Count: 4640\n",
            "Saved extracted information to /content/json_output/34657584.json\n",
            "--------------------------------------------------\n",
            "Information Extraction Completed.\n",
            "Total time taken: 1528.4474494457245 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xPjzWEeN6EYG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}